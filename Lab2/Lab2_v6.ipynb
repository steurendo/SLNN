{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwhGEjuxTlZz"
      },
      "source": [
        "**STATISTICAL LEARNING AND NEURAL NETWORKS, A.A. 2022/2023**\n",
        "\n",
        "**COMPUTER LAB 2** - Model fitting and classification\n",
        "\n",
        "**Duration: 6 hours**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw1lJAyRTtfy"
      },
      "source": [
        "**Exercise 1 – Model fitting for continuous distributions: Multivariate Gaussian**\n",
        "\n",
        "In this exercise, you will employ a dataset based on the classic dataset *Iris Plants Database* https://archive.ics.uci.edu/ml/datasets/iris. You will be provided a subset of this dataset comprising only two classes (*Iris Setosa* and *Iris Versicolour*), and only two features per class (*petal length* in cm and *petal width* in cm). The objective is to the determine the kind of iris based on the content of the features.\n",
        "\n",
        "**Task:** you have to fit class-conditional Gaussian multivariate distributions to the data, and visualize the probability density functions. In particular, you should perform the following:\n",
        "\n",
        "\n",
        "*   Divide the dataset in two parts (*Iris Setosa* which corresponds to class zero and *Iris Versicolour* which correspond to class 1). Then work only on one class at a time.\n",
        "*   Plot the data of each class (use the *plt.scatter( )* function)\n",
        "*   Visualize the histogram of petal length and petal width (use e.g. the *plt.hist( )* function)\n",
        "*   Calculate the maximum likelihood estimate of the mean and covariance matrix under a multivariate Gaussian model, independently for each class (these are the parameters of the class-conditional distributions). Note: is the Gaussian model good for these data?\n",
        "*    Visualize the 2-D joint pdf of petal length and width for the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpkQuQS3TsZI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "Dataset1 = h5py.File(\"/content/Lab2_Ex_1_Iris.hdf5\")\n",
        "Data = np.array(Dataset1.get('Dataset'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4nPKMp-bTQa"
      },
      "outputs": [],
      "source": [
        "#Separate the dataset in the two classes, you can use the numpy function argsort and unique to do this.  \n",
        "#Draw the scatter plot of the two classes on the same image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "620rAMG0depm"
      },
      "outputs": [],
      "source": [
        "#Visualize the histogram of petal length and petal width (use e.g. the plt.hist() function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGm5v5fnfxE9"
      },
      "outputs": [],
      "source": [
        "#Calculate mean and covariance matrix under a multivariate Gaussian model. Scalar products can be computed with the function np.matmul()\n",
        "\n",
        "#The transpose can be obtained with the function np.transpose \n",
        "\n",
        "#To make a scalar product between arrays in the form [Mx1]x[1xM] starting from 1D array A it may be necessary to add a new axis using \n",
        "#the function A[:,np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF9GkeLT9Dlc"
      },
      "outputs": [],
      "source": [
        "#Visualize the 2-D joint pdf of petal length and width, a pdf function can be initialized by providing mean and covariance matrix\n",
        "from scipy.stats import multivariate_normal\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator\n",
        "\n",
        "#pdf_class0 = multivariate_normal(mean=..., cov=...)\n",
        "#pdf_class1 = multivariate_normal(mean=..., cov=...)\n",
        "\n",
        "#Create a grid of x and y values on which to sample the pdf, this is done by providing a list of x-y of coordinates to the function pdf_class0.pdf(...)\n",
        "#A 3D view of the pdf can be obtained using the function ax.plot_surface\n",
        "\n",
        "#Code Example:\n",
        "\n",
        "#X = np.arange(...)\n",
        "#Y = np.arange(...)\n",
        "\n",
        "#X, Y = np.meshgrid(X, Y)\n",
        "#X_flat = X.flatten()\n",
        "#Y_flat = Y.flatten()\n",
        "#XY_list = np.concatenate((X_flat[:,np.newaxis],Y_flat[:,np.newaxis]),axis=1)\n",
        "#PDF_values = np.reshape(pdf_class0.pdf(XY_list), np.shape(X))\n",
        "\n",
        "#fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"},figsize=(8, 8), dpi=150)\n",
        "#ax.plot_surface(X, Y, PDF_values, cmap=cm.coolwarm, alpha=0.7, linewidth=0)\n",
        "\n",
        "#To change the orientation of the 3D plot the function ax.view_init(), for a view from above select ax.view_init( 90, 0)\n",
        "\n",
        "#After visualizing the pdf you can plot the points of the dataset on the estimated pdf using ax.scatter3D()\n",
        "#For a better visualization of the points we suggest to make the pdf plot semi-transparent using the alpha parameter\n",
        "\n",
        "#Code Example: \n",
        "#PDF_points_class0 = pdf_class0.pdf(Features_class0)\n",
        "#ax.scatter3D(Points_Class0_Feature0, Points_Class0_Feature1, PDF_points_class0, s=10)\n",
        "\n",
        "#Note: the sample code was written only for class 0 but two plots have to be done, one for class 0 and one for class 1\n",
        "#Note 2: the content of variables like Features_class0, Points_Class0_Feature0, Points_Class0_Feature1 should be substituted with variables created by the student"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTpZd04_NEZz"
      },
      "source": [
        "**Student's comments to Exercise 1**\n",
        "\n",
        "*Add comments to the results of Exercise 1 here (may use LateX for formulas if needed).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NemN6SgT9zn"
      },
      "source": [
        "**Exercise 2 - Model fitting for discrete distributions: Bag of Words**\n",
        "\n",
        "\n",
        "In this exercise, you will employ a real dataset (file *SMSSpamCollection*). The SMS Spam Collection v.1 (https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
        "Task: you have to fit the parameters employed by a Naïve Bayes Classifier, using a Bernoulli model. Under this model, the parameters are:\n",
        "\n",
        "*   $\\pi_{c}$, the prior probabilities of each class.\n",
        "*   $\\theta_{jc}$, the probability that feature j is equal to 1 in class c.\n",
        "\n",
        "Model fitting can be done using the pseudocode at the end of the Lecture 3 slides.\n",
        "\n",
        "Display the class-conditional densities $\\theta_{jc1}$ and $\\theta_{jc2}$. Try to identify “uninformative” features (i.e., features j such that $\\theta_{jc1}$ ≃ $\\theta_{jc2}$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "aRuVqGZZck1K",
        "outputId": "6b7a83b9-7c6e-4a24-8776-91b891389ad8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# reading the data\n",
        "data = pd.read_csv(\"SMSSpamCollection\", encoding=\"ISO-8859-1\", sep=\"\\t\", header=None)\n",
        "data.rename(columns={0: \"labels\", 1: \"text\"}, inplace=True)\n",
        "display(data)\n",
        "\n",
        "# Transform data to bag of word representation\n",
        "bagger = CountVectorizer(\n",
        "    max_features=2500,\n",
        "    binary=True,  # Bernulli Model\n",
        "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\",\n",
        ")\n",
        "bag = bagger.fit_transform(data[\"text\"]).toarray()\n",
        "feature_names = bagger.get_feature_names_out()\n",
        "\n",
        "data = pd.concat([data, pd.DataFrame(bag, columns=feature_names)], axis=1)\n",
        "display(data)\n",
        "\n",
        "X_train = data.iloc[:2000,2:].to_numpy()\n",
        "X_test = data.iloc[2000:3000,2:].to_numpy()\n",
        "y_train = data.iloc[:2000,0].to_numpy() == 'ham'\n",
        "y_test = data.iloc[2000:3000,0].to_numpy() == 'ham'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYqtwjJZwvej"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDc-RTLPUn_g"
      },
      "outputs": [],
      "source": [
        "#Evaluate the probabilities of the two classes, and the class conditional densities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_o62zfqTCR3"
      },
      "outputs": [],
      "source": [
        "#Display the class-conditional densities θjc1 and θjc2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo67SKuYNN2z"
      },
      "source": [
        "**Student's comments to Exercise 2**\n",
        "\n",
        "*Add comments to the results of Exercise 2 here (may use LateX for formulas if needed).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnWIukCTcnzE"
      },
      "source": [
        "**Exercise 3 - Classification – discrete data**\n",
        "\n",
        "In this exercise, you will design a Naïve Bayes Classifier (NBC) for the Bag of Words (BoW) features for document classification that have been prepared in *Exercise 2*. In particular, in *exercise 2*, you have already estimated the following parameters:\n",
        "\n",
        "\n",
        "*   The prior probabilities of each class, $\\pi_{c} = p(y=c)$.\n",
        "*   The class-conditional probabilities of each feature, $\\theta_{jc} = p(x_{j}=1 | y=c)$.\n",
        "\n",
        "These parameters have been estimated from the training data. In this exercise, you will use the test data, and classify each test vector using an NBC whose model has been fitted in *Exercise 2*. In particular, you will do the following:\n",
        "\n",
        "\n",
        "\n",
        "*   For each test vector, calculate the MAP estimate of the class the test vector belongs to. Remember: the MAP classifier chooses the class that maximizes $\\mathop{\\max\\limits_c{\\log p(y=c|x)} \\propto \\log p(x|y=c)} + \\log p(c)$. In the NBC, the features (i.e. each entry of $x$) are assumed to be statistically independent, so $p(x|y=c) = \\prod_{j=1}^{D}p(x_{j}|y=c)$. This formula allows you to calculate $p(x|y=c)$ for a given test vector $x$ using the parameters $\\theta_{jc}$ already calculated in *Exercise 2*. Note that, after the logarithm, the product \n",
        "becomes a summation. It is much better to use the logarithm in order to avoid underflow.\n",
        "*   See how the accuracy changes when the prior is not taken into account (e.g. by comparing the MLE and MAP estimate).\n",
        "*   After classifying a test vector using the NBC, the obtained class can be compared with the truth (vector *ytest*).\n",
        "*   The accuracy of the classifier can be computed as the percentage of times that the NBC provides the correct class for a test vector.\n",
        "*   Repeat the same operations using the training data as test data, and compare the accuracy of the classifier on the training and test data.\n",
        "*   Note: It is expected that students implement the Naive Bayes classifier from scratch without using pre-made functions such as sklearn.naive_bayes\n",
        "\n",
        "**Optional:**\n",
        "\n",
        "If you plot the class-conditional densities as done at the end of Exercise 2, you will see that many features are uninformative; e.g., words that appear very often (or very rarely) in documents belonging to either class are not very helpful to classify a document. The NBC can perform a lot better if these uninformative features are disregarded during the classification, i.e. only a subset of the features, chosen among the most informative ones, are retained. To rank the features by “significance”, one can employ the mutual information between feature $x_{j}$ and class $y$ (see Sec. 3.5.4 of the textbook):\n",
        "\n",
        "\\begin{align*}\n",
        "I(X,Y) = \\sum_{xj} \\sum_{y}p(x_{j},y) \\log \\frac{p(x_{j},y)}{p(x_{j})p(y)}\n",
        "\\end{align*}\n",
        "\n",
        "For binary features, the mutual information of feature j can be written as:\n",
        "\n",
        "\\begin{align*}\n",
        "I_{j} = \\sum_{c} \\left[\\theta_{jc}\\pi_{c}\\log \\frac{\\theta_{jc}}{\\theta_{j}}+ (1 - \\theta_{jc})\\pi_{c} \\log \\frac{1 - \\theta_{jc}}{1 - \\theta_{j}}\\right]\n",
        "\\end{align*}\n",
        "\n",
        "with $\\theta_{j}=p(x_{j}=1)=\\sum_{c}\\pi_{c}\\theta_{jc}$. For this part, you should:\n",
        "\n",
        "\n",
        "*   Calculate $I_{j}$ for all features. Note: try to avoid divisions by zero adding the machine precision constant *eps* to the denominators.\n",
        "*   Rank the features by decreasing values of $I_{j}$, and keep only the $K$ most important ones.\n",
        "\n",
        "*    Run the classifier employing only the $K$ most important features, and calculate the accuracy.\n",
        "\n",
        "*    Plot the accuracy as a function of $K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q0Cbwtmcm8D"
      },
      "outputs": [],
      "source": [
        "#Evaluate the MAP on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hFwaEmsZxxq"
      },
      "outputs": [],
      "source": [
        "#Evaluate the MLE on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PTAw1dMagCR"
      },
      "outputs": [],
      "source": [
        "#Optional section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkAjnxUxNWc3"
      },
      "source": [
        "**Student's comments to Exercise 3**\n",
        "\n",
        "*Add comments to the results of Exercise 3 here (may use LateX for formulas if needed).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QWIOOdBc0R5"
      },
      "source": [
        "**Exercise 4 – Plotting the ROC curve**\n",
        "\n",
        "For the discrete data classification problem of Exercise 3, analyse the performance of the classifier plotting the complete ROC curve, instead of simply measuring the accuracy. This requires to do the following.\n",
        "\n",
        "*   Instead of classifying the documents choosing class 1 if $p(y=1|x)>p(y=2|x)$, now you can generalize this to choosing class 1 if $\\frac{p(y=1|x)}{p(y=2|x)}>\\tau \\in [0, \\infty)$ for some threshold $\\tau$ that determines the compromise between true positive rate (TPR) and false positive rate (FPR). \n",
        "*   Choose a **reasonable** range of values for $\\tau$. For each value of $\\tau$, compute the TPR and FPR on the dataset (Hint: determine suitable minimum and maximum values for $\\tau$, and sample densely enough in that range). \n",
        "* Plot a curve of TPR as a function of FPR – this is the ROC curve for this classifier.\n",
        "\n",
        "*   Determine an estimate of the Equal Error Rate (EER), i.e. the point of the ROC curve such that TPR+FPR=1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btE3PLH-c9Q3"
      },
      "outputs": [],
      "source": [
        "#Use the posterior probabilities previously and classify using the formula above to estimate tau\n",
        "\n",
        "#Note: To estimate the TPR you need to compute the number of cases where class 1 is correctly predicted, \n",
        "#this value then has to be divided by the number of elements in the test set which belong to class 1 \n",
        "\n",
        "#The FPR is computed by selecting the number of cases where class 1 is predicted incorrectly, \n",
        "#this value then has to be divided by the number of elements in the test set which do not belong to class 1  \n",
        "\n",
        "#The point corresponding to the EER can be found by plotting on the ROC curve the function y = 1 - x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Me2HlCRNa8W"
      },
      "source": [
        "**Student's comments to Exercise 4**\n",
        "\n",
        "*Add comments to the results of Exercise 4 here (may use LateX for formulas if needed).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQNUFexxc94-"
      },
      "source": [
        "**Exercise 5 – Classification – continuous data**\n",
        "\n",
        "This exercise employs the Iris dataset already employed in Exercise 1, and performs model fitting and classification using several versions of **Gaussian discriminative analysis**. However, for this exercise the available data have to be divided into two sets, namely *training* and *test* data. \n",
        "\n",
        "You will have to 1) re-fit the training data to the specific model (see below), 2) classify each of the test samples, and 3) calculate the accuracy of each classifier.\n",
        "\n",
        "Classifiers to be employed:\n",
        "*   Two-class quadratic discriminant analysis (fitting: both mean values and covariance matrices are class-specific – same as in exercise 1).\n",
        "*   Two-class linear discriminant analysis (fitting: class-specific mean values as in the previous case. Shared covariance matrix is calculated putting together the elements of both classes; the mean values should also be recalculated accordingly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJTl-QmJdBDj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "\n",
        "Dataset1 = h5py.File(\"/content/Lab2_Ex_1_Iris.hdf5\")\n",
        "Data = np.array(Dataset1.get('Dataset')) \n",
        "\n",
        "Train = Data[:50,:]\n",
        "Test = Data[50:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp32RGD6Nd2K"
      },
      "source": [
        "**Student's comments to Exercise 5**\n",
        "\n",
        "*Add comments to the results of Exercise 5 here (may use LateX for formulas if needed).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-3cCP8ndBkz"
      },
      "source": [
        "**Exercise 6 – Classification – continuous data**\n",
        "\n",
        "Classify the data in the phoneme dataset from Lab. 1 using quadratic discriminant analysis, linear discriminat analysis and a Naive Bayes classifier. \n",
        "\n",
        "Compute the accuracy of each classifier and compare its performance with that of the k-nn classifier developed in Lab. 1.\n",
        "\n",
        "Note: in Lab1 we had to employ a subset of the oiginal dataset due to the fact that k-nn has a quadratic complexity making it unfit for use on large datasets. The algorithms illustrated in this Lab have smaller complexity and thus it is possible to train on more data.\n",
        "\n",
        "For this exercise you can use the sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7urLt_kdLxt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "Dataset2 = h5py.File(\"/content/Lab2_Ex_6_phoneme.hdf5\")\n",
        "Data = np.array(Dataset2.get('Dataset'))\n",
        "\n",
        "Train = Data[:4000,:]\n",
        "Test = Data[4000:,:]\n",
        "len_dat = np.shape(Test)[0]\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#Part 1\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "#Complete here\n",
        "\n",
        "#Part 2\n",
        "clf2 = QuadraticDiscriminantAnalysis()\n",
        "#Complete here\n",
        "\n",
        "#Part 3\n",
        "clf3 = GaussianNB()\n",
        "#Complete here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faJ5zulmNhAJ"
      },
      "source": [
        "**Student's comments to Exercise 6**\n",
        "\n",
        "*Add comments to the results of Exercise 6 here (may use LateX for formulas if needed).*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
